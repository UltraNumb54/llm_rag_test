import pandas as pd
import numpy as np
from transformers import (
    AutoTokenizer, 
    AutoModelForTokenClassification, 
    TrainingArguments, 
    Trainer,
    DataCollatorForTokenClassification
)
from datasets import Dataset, load_metric
from seqeval.metrics import classification_report, f1_score
import torch

# Параметры
MODEL_NAME = "DeepPavlov/rubert-base-cased"
BATCH_SIZE = 8  # Adjust based on your GPU memory
LEARNING_RATE = 2e-5
NUM_EPOCHS = 3
MAX_LENGTH = 256

# Загрузка токенизатора
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Функция для преобразования в BIO-разметку
def convert_to_bio(original_text, annotated_text, labels):
    """
    Преобразует текст с метками в BIO-разметку.
    original_text: исходный текст
    annotated_text: текст с метками [FIO], [LOC], и т.д.
    labels: список меток (например, ['FIO', 'LOC', 'DATE'])
    Возвращает список токенов и список меток в BIO-формате.
    """
    words = original_text.split()
    bio_labels = ['O'] * len(words)
    
    # Создаем копию аннотированного текста для работы
    temp_annotated = annotated_text
    
    # Для каждой метки находим её позицию и заменяем на placeholder
    for label in labels:
        start_tag = f"[{label}]"
        while start_tag in temp_annotated:
            start_idx = temp_annotated.find(start_tag)
            if start_idx == -1:
                break
                
            # Находим конец метки
            end_idx = start_idx + len(start_tag)
            
            # Текст до метки
            text_before = temp_annotated[:start_idx].strip()
            # Количество слов до метки
            word_count_before = len(text_before.split()) if text_before else 0
            
            # Текст после метки
            text_after = temp_annotated[end_idx:].strip()
            
            # Находим соответствующую последовательность в оригинальном тексте
            original_words = original_text.split()
            annotated_words_before = text_before.split()
            annotated_words_after = text_after.split()
            
            # Находим начало и конец сущности в оригинальном тексте
            entity_start = len(annotated_words_before)
            entity_end = len(original_words) - len(annotated_words_after)
            
            if entity_start < entity_end:
                # Размечаем BIO
                bio_labels[entity_start] = f'B-{label}'
                for i in range(entity_start + 1, entity_end):
                    bio_labels[i] = f'I-{label}'
            
            # Заменяем обработанную метку на пробелы для продолжения
            temp_annotated = temp_annotated.replace(start_tag, ' ', 1)
    
    return words, bio_labels

# Загрузка и предобработка данных
def load_and_preprocess_data(csv_file):
    df = pd.read_csv(csv_file)
    
    labels = ['FIO', 'LOC', 'DATE']  # Ваши метки
    
    all_tokens = []
    all_labels = []
    
    for _, row in df.iterrows():
        original_text = row['original_message']
        annotated_text = row['annotated_message']
        
        tokens, bio_labels = convert_to_bio(original_text, annotated_text, labels)
        
        all_tokens.append(tokens)
        all_labels.append(bio_labels)
    
    return all_tokens, all_labels, labels

# Загрузка данных
csv_file = "your_dataset.csv"  # Укажите путь к вашему CSV файлу
all_tokens, all_labels, unique_labels = load_and_preprocess_data(csv_file)

# Создание label2id и id2label
label_list = ['O']
for label in unique_labels:
    label_list.append(f'B-{label}')
    label_list.append(f'I-{label}')

label2id = {label: i for i, label in enumerate(label_list)}
id2label = {i: label for i, label in enumerate(label_list)}

# Токенизация и выравнивание меток
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"],
        truncation=True,
        padding=False,
        is_split_into_words=True,
        max_length=MAX_LENGTH,
        return_offsets_mapping=True
    )
    
    labels = []
    for i, label in enumerate(examples["labels"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label2id.get(label[word_idx], -100))
            else:
                # Токены, являющиеся частью одного слова
                current_label = label[word_idx]
                if current_label.startswith('B-'):
                    # Превращаем в I- метку для последующих токенов того же слова
                    label_ids.append(label2id.get('I-' + current_label[2:], -100))
                else:
                    label_ids.append(label2id.get(current_label, -100))
            previous_word_idx = word_idx
        
        labels.append(label_ids)
    
    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Создание datasets
data = {
    "tokens": all_tokens,
    "labels": all_labels
}
dataset = Dataset.from_dict(data)

# Разделение на train/validation
train_test_split = dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = train_test_split["train"]
eval_dataset = train_test_split["test"]

# Токенизация datasets
tokenized_train_dataset = train_dataset.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=train_dataset.column_names
)

tokenized_eval_dataset = eval_dataset.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=eval_dataset.column_names
)

# Data collator
data_collator = DataCollatorForTokenClassification(
    tokenizer=tokenizer,
    padding=True
)

# Загрузка модели
model = AutoModelForTokenClassification.from_pretrained(
    MODEL_NAME,
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True
)

# Метрики
metric = load_metric("seqeval")

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }

# Аргументы обучения
training_args = TrainingArguments(
    output_dir="./ner_model",
    learning_rate=LEARNING_RATE,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    num_train_epochs=NUM_EPOCHS,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=False,
    logging_dir="./logs",
    report_to=None,
    fp16=torch.cuda.is_available(),  # Включить, если GPU поддерживает mixed precision
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Запуск обучения
trainer.train()

# Сохранение модели
trainer.save_model("./ner_final_model")
tokenizer.save_pretrained("./ner_final_model")

# Инференс на примере
def predict_entities(text, model, tokenizer, label_list):
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=MAX_LENGTH,
        return_offsets_mapping=True
    )
    
    with torch.no_grad():
        outputs = model(**inputs)
    
    predictions = torch.argmax(outputs.logits, dim=2)[0].cpu().numpy()
    
    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
    offsets = inputs["offset_mapping"][0]
    
    entities = []
    current_entity = None
    
    for i, (token, offset, pred) in enumerate(zip(tokens, offsets, predictions)):
        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:
            continue
            
        label = label_list[pred]
        
        if label.startswith('B-'):
            if current_entity is not None:
                entities.append(current_entity)
            current_entity = {
                'entity': label[2:],
                'start': offset[0],
                'end': offset[1],
                'word': tokenizer.decode(inputs["input_ids"][0][i])
            }
        elif label.startswith('I-'):
            if current_entity is not None and current_entity['entity'] == label[2:]:
                current_entity['end'] = offset[1]
                current_entity['word'] += tokenizer.decode(inputs["input_ids"][0][i]).replace('##', '')
            else:
                if current_entity is not None:
                    entities.append(current_entity)
                current_entity = None
        else:
            if current_entity is not None:
                entities.append(current_entity)
                current_entity = None
    
    if current_entity is not None:
        entities.append(current_entity)
    
    return entities

# Тестирование
test_text = "Иванов Иван поехал в город Коряжма и купил в мае 2026 или 03.05.2026."
entities = predict_entities(test_text, model, tokenizer, label_list)
print("Извлеченные сущности:", entities)
