from fastapi import APIRouter, HTTPException, Depends
from app.models.schemas import (
    QueryRequest, QueryResponse, IngestRequest, IngestResponse, 
    ConfigRequest, HealthResponse
)
from app.services.llm_service import LLMService
from app.services.embedding_service import EmbeddingService
from app.services.vector_store import VectorStore
from app.config import settings
import os
import glob
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import CrossEncoder
import logging

logger = logging.getLogger(__name__)

router = APIRouter()

# Глобальные сервисы (будут инициализированы в main.py)
llm_service = None
embedding_service = None
vector_store = None
reranker = None

@router.post("/configure", response_model=dict)
async def configure_model(config: ConfigRequest):
    """Настройка LLM модели"""
    global llm_service
    try:
        llm_service = LLMService(
            base_url=config.model_base_url,
            api_key=config.model_api_key
        )
        # Тестируем подключение
        if llm_service.check_connection():
            return {"status": "success", "message": f"Модель настроена на {config.model_base_url}"}
        else:
            raise HTTPException(status_code=500, detail="Не удалось подключиться к модели")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ошибка подключения к модели: {str(e)}")

def load_documents_from_directory(directory_path: str):
    """Загрузка текстовых документов из директории"""
    documents = []
    
    # Ищем все txt файлы
    pattern = os.path.join(directory_path, "**/*.txt") if os.path.isdir(directory_path) else directory_path
    files = glob.glob(pattern, recursive=True)
    
    for file_path in files:
        try:
            if file_path.endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    documents.append({
                        'content': content,
                        'source': file_path
                    })
                    logger.info(f"Загружен файл: {file_path}")
        except Exception as e:
            logger.error(f"Ошибка загрузки файла {file_path}: {e}")
    
    return documents

@router.post("/ingest", response_model=IngestResponse)
async def ingest_documents(ingest: IngestRequest):
    """Загрузка документов в векторную БД"""
    if embedding_service is None or vector_store is None:
        raise HTTPException(status_code=400, detail="Сервисы не инициализированы")
    
    if not os.path.exists(ingest.documents_path):
        raise HTTPException(status_code=400, detail="Указанный путь не существует")
    
    try:
        # Загрузка документов
        documents = load_documents_from_directory(ingest.documents_path)
        if not documents:
            return IngestResponse(
                status="warning", 
                message="Не найдено документов для обработки",
                documents_count=0,
                chunks_count=0
            )
        
        # Разбивка на чанки
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=ingest.chunk_size,
            chunk_overlap=ingest.chunk_overlap
        )
        
        all_chunks = []
        for doc in documents:
            chunks = text_splitter.split_text(doc['content'])
            for chunk in chunks:
                all_chunks.append({
                    'content': chunk,
                    'source': doc['source']
                })
        
        # Генерация эмбеддингов
        texts = [chunk['content'] for chunk in all_chunks]
        embeddings = embedding_service.encode(texts)
        
        # Добавление в коллекцию
        vector_store.add_documents(
            embeddings=embeddings,
            documents=texts,
            metadatas=[{"source": chunk['source']} for chunk in all_chunks],
            ids=[f"doc_{i}" for i in range(len(texts))]
        )
        
        return IngestResponse(
            status="success",
            message=f"Загружено {len(documents)} документов, создано {len(texts)} чанков",
            documents_count=len(documents),
            chunks_count=len(texts)
        )
        
    except Exception as e:
        logger.error(f"Ошибка обработки документов: {e}")
        raise HTTPException(status_code=500, detail=f"Ошибка обработки документов: {str(e)}")

@router.post("/ask", response_model=QueryResponse)
async def ask_question(query: QueryRequest):
    """Задать вопрос системе RAG"""
    if llm_service is None or embedding_service is None or vector_store is None:
        raise HTTPException(status_code=400, detail="Сервисы не инициализированы")
    
    if vector_store.count() == 0:
        raise HTTPException(status_code=400, detail="Векторная БД пуста. Сначала загрузите документы")
    
    try:
        # Поиск релевантных документов
        query_embedding = embedding_service.encode([query.question])[0]
        results = vector_store.search(query_embedding, n_results=query.top_k)
        
        if not results['documents']:
            return QueryResponse(
                question=query.question,
                answer="К сожалению, в моих документах нет информации для ответа на этот вопрос.",
                sources=[],
                total_chunks_in_db=vector_store.count()
            )
        
        documents = results['documents'][0]
        metadatas = results['metadatas'][0]
        
        # Переранжирование (опционально)
        if reranker and len(documents) > 1:
            pairs = [(query.question, doc) for doc in documents]
            rerank_scores = reranker.predict(pairs, show_progress_bar=False)
            combined_results = list(zip(documents, metadatas, rerank_scores))
            combined_results.sort(key=lambda x: x[2], reverse=True)
            final_results = combined_results[:query.rerank_top_k]
        else:
            final_results = [(doc, meta, 1.0) for doc, meta in zip(documents, metadatas)]
        
        # Генерация ответа
        context = "\n\n".join([doc[0] for doc in final_results])
        
        prompt = f"""Используй только приведённый ниже контекст для ответа на вопрос. Если в контексте нет информации для ответа, скажи об этом.

Контекст:
{context}

Вопрос: {query.question}

Ответ:"""
        
        answer = llm_service.generate_response(prompt)
        
        return QueryResponse(
            question=query.question,
            answer=answer,
            sources=[{"content": doc[0], "source": doc[1]['source'], "score": float(doc[2])} for doc in final_results],
            total_chunks_in_db=vector_store.count()
        )
        
    except Exception as e:
        logger.error(f"Ошибка обработки запроса: {e}")
        raise HTTPException(status_code=500, detail=f"Ошибка обработки запроса: {str(e)}")

@router.get("/health", response_model=HealthResponse)
async def health_check():
    """Проверка здоровья системы"""
    return HealthResponse(
        status="healthy",
        device=embedding_service.device if embedding_service else "unknown",
        collection_size=vector_store.count() if vector_store else 0,
        model_configured=llm_service is not None,
        embedding_model_ready=embedding_service is not None
    )

@router.get("/stats")
async def get_stats():
    """Получить статистику системы"""
    return {
        "vector_db_documents": vector_store.count() if vector_store else 0,
        "embedding_model": settings.EMBEDDING_MODEL if embedding_service else None,
        "llm_configured": llm_service is not None
    }
