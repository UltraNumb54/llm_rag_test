files
import os
import asyncio
import aiofiles
from app.config import settings
from app.services.file_processor import FileProcessor
from app.services.vector_store import vector_store
from fastapi import APIRouter, BackgroundTasks, File, HTTPException, UploadFile
from loguru import logger

router = APIRouter()
file_processor = FileProcessor()

async def process_file_background(file_path: str, filename: str):
    """Асинхронная фоновая обработка файла"""
    try:
        logger.info(f"Начата фоновая обработка файла: {filename}")
        
        # Обрабатываем файл и получаем документы
        documents = await file_processor.process_file(file_path, filename)
        
        if documents:
            # Добавляем документы в векторное хранилище
            await vector_store.add_documents(documents)
            logger.success(f"Файл {filename} успешно обработан, добавлено {len(documents)} документов")
        else:
            logger.warning(f"Файл {filename} не содержит извлекаемого текста")
            
    except Exception as e:
        logger.error(f"Ошибка обработки файла {filename}: {e}")
    finally:
        # Очищаем временный файл
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                logger.debug(f"Временный файл {file_path} удален")
        except Exception as e:
            logger.warning(f"Не удалось удалить временный файл {file_path}: {e}")

@router.post("/upload")
async def upload_file(background_tasks: BackgroundTasks, file: UploadFile = File(...)):
    try:
        # Проверяем размер файла
        content = await file.read()
        if len(content) > settings.MAX_FILE_SIZE:
            raise HTTPException(
                status_code=400,
                detail=f"Файл слишком большой. Максимальный размер: {settings.MAX_FILE_SIZE // (1024 * 1024)}MB",
            )

        # Сохраняем файл
        upload_dir = "uploads"
        os.makedirs(upload_dir, exist_ok=True)
        file_path = os.path.join(upload_dir, file.filename)

        async with aiofiles.open(file_path, "wb") as f:
            await f.write(content)

        # Запускаем фоновую задачу с правильным вызовом
        background_tasks.add_task(process_file_background, file_path, file.filename)

        return {
            "status": "success",
            "filename": file.filename,
            "message": "Файл загружен и отправлен на обработку",
        }

    except Exception as e:
        logger.error(f"Ошибка загрузки файла: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/files")
async def list_files():
    """Список загруженных файлов"""
    try:
        upload_dir = "uploads"
        if not os.path.exists(upload_dir):
            return {"files": []}

        files = []
        for filename in os.listdir(upload_dir):
            file_path = os.path.join(upload_dir, filename)
            stats = os.stat(file_path)
            files.append(
                {
                    "filename": filename,
                    "size": stats.st_size,
                    "uploaded_at": stats.st_ctime,
                }
            )

        return {"files": files}
    except Exception as e:
        logger.error(f"Ошибка получения списка файлов: {e}")
        raise HTTPException(status_code=500, detail=str(e))

file processor

import os
import asyncio
from typing import List
from loguru import logger

class FileProcessor:
    async def process_file(self, file_path: str, filename: str) -> List[str]:
        """Асинхронная обработка файла и извлечение текста"""
        try:
            logger.info(f"Обработка файла: {filename}")
            
            # Определяем тип файла по расширению
            ext = os.path.splitext(filename)[1].lower()
            
            if ext == '.txt':
                return await self._process_txt(file_path)
            elif ext == '.pdf':
                return await self._process_pdf(file_path)
            elif ext in ['.doc', '.docx']:
                return await self._process_docx(file_path)
            elif ext == '.md':
                return await self._process_markdown(file_path)
            else:
                logger.warning(f"Неподдерживаемый формат файла: {ext}")
                return []
                
        except Exception as e:
            logger.error(f"Ошибка обработки файла {filename}: {e}")
            return []

    async def _process_txt(self, file_path: str) -> List[str]:
        """Обработка текстовых файлов"""
        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                content = await f.read()
            
            if not content.strip():
                return []
                
            # Разбиваем на чанки
            chunks = self._split_text(content)
            logger.info(f"Текстовый файл обработан, создано {len(chunks)} чанков")
            return chunks
            
        except Exception as e:
            logger.error(f"Ошибка чтения текстового файла: {e}")
            return []

    async def _process_pdf(self, file_path: str) -> List[str]:
        """Обработка PDF файлов"""
        try:
            # Для PDF используем синхронную библиотеку, но запускаем в отдельном потоке
            return await asyncio.get_event_loop().run_in_executor(
                None, self._process_pdf_sync, file_path
            )
        except Exception as e:
            logger.error(f"Ошибка обработки PDF: {e}")
            return []

    def _process_pdf_sync(self, file_path: str) -> List[str]:
        """Синхронная обработка PDF"""
        try:
            import PyPDF2
            
            chunks = []
            with open(file_path, 'rb') as f:
                pdf_reader = PyPDF2.PdfReader(f)
                
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text = page.extract_text()
                    
                    if text.strip():
                        page_chunks = self._split_text(text)
                        chunks.extend(page_chunks)
            
            logger.info(f"PDF файл обработан, создано {len(chunks)} чанков")
            return chunks
            
        except ImportError:
            logger.error("PyPDF2 не установлен. Установите: pip install PyPDF2")
            return []
        except Exception as e:
            logger.error(f"Ошибка синхронной обработки PDF: {e}")
            return []

    async def _process_docx(self, file_path: str) -> List[str]:
        """Обработка DOCX файлов"""
        try:
            return await asyncio.get_event_loop().run_in_executor(
                None, self._process_docx_sync, file_path
            )
        except Exception as e:
            logger.error(f"Ошибка обработки DOCX: {e}")
            return []

    def _process_docx_sync(self, file_path: str) -> List[str]:
        """Синхронная обработка DOCX"""
        try:
            import docx
            
            doc = docx.Document(file_path)
            full_text = []
            
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    full_text.append(paragraph.text)
            
            text = '\n'.join(full_text)
            chunks = self._split_text(text)
            
            logger.info(f"DOCX файл обработан, создано {len(chunks)} чанков")
            return chunks
            
        except ImportError:
            logger.error("python-docx не установлен. Установите: pip install python-docx")
            return []
        except Exception as e:
            logger.error(f"Ошибка синхронной обработки DOCX: {e}")
            return []

    async def _process_markdown(self, file_path: str) -> List[str]:
        """Обработка Markdown файлов"""
        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                content = await f.read()
            
            # Упрощенная обработка Markdown - удаляем разметку
            import re
            text = re.sub(r'#+\s?', '', content)  # Удаляем заголовки
            text = re.sub(r'\*{1,2}(.*?)\*{1,2}', r'\1', text)  # Удаляем жирный/курсив
            text = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', text)  # Удаляем ссылки
            
            chunks = self._split_text(text)
            logger.info(f"Markdown файл обработан, создано {len(chunks)} чанков")
            return chunks
            
        except Exception as e:
            logger.error(f"Ошибка обработки Markdown: {e}")
            return []

    def _split_text(self, text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[str]:
        """Разбивка текста на перекрывающиеся чанки"""
        from app.config import settings
        
        chunk_size = settings.CHUNK_SIZE
        chunk_overlap = settings.CHUNK_OVERLAP
        
        if len(text) <= chunk_size:
            return [text] if text.strip() else []
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # Если не конец текста, пытаемся закончить на границе предложения
            if end < len(text):
                # Ищем последнюю точку, восклицательный или вопросительный знак
                sentence_end = max(
                    text.rfind('.', start, end),
                    text.rfind('!', start, end),
                    text.rfind('?', start, end),
                    text.rfind('\n', start, end)
                )
                
                if sentence_end > start and sentence_end - start > chunk_size // 2:
                    end = sentence_end + 1
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - chunk_overlap
            
            # Защита от бесконечного цикла
            if start >= len(text):
                break
        
        return chunks


chat.py

from typing import List, Optional
from app.config import settings
from app.services.llm_service import llm_service
from app.services.reranker import reranker_service
from app.services.vector_store import vector_store
from fastapi import APIRouter, HTTPException
from loguru import logger
from pydantic import BaseModel, Field

router = APIRouter()

# Временное хранилище для истории диалогов
conversation_store = {}

class ChatRequest(BaseModel):
    message: str
    conversation_id: Optional[str] = None
    use_reranking: bool = True

class ChatResponse(BaseModel):
    response: str = Field(default="")
    conversation_id: str = Field(default="default")
    sources: List[str] = Field(default_factory=list)
    suggested_questions: List[str] = Field(default_factory=list)

@router.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    try:
        logger.info(f"Получен запрос чата: {request.message}")

        # Создаем или получаем ID диалога
        conversation_id = request.conversation_id or f"conv_{len(conversation_store) + 1}"

        # Получаем историю диалога
        history = conversation_store.get(conversation_id, [])

        # Ищем релевантные документы с AWAIT
        search_results = await vector_store.search(request.message, top_k=settings.TOP_K)
        logger.info(f"Найдено документов: {len(search_results)}")

        response_text = ""
        sources = []

        if not search_results:
            response_text = (
                "Извините, в моей базе знаний нет информации для ответа на этот вопрос."
            )
            logger.info("Документы не найдены, используем стандартный ответ")
        else:
            # Извлекаем тексты документов
            documents = [result["document"] for result in search_results]
            logger.info(f"Первый документ: {documents[0][:100]}..." if documents else "Нет документов")

            # Реранкинг если включен
            if request.use_reranking and len(documents) > 1:
                logger.info("Применяем реранкинг документов")
                reranked_docs = reranker_service.rerank(
                    request.message, documents, top_k=settings.RERANK_TOP_K
                )
                context_docs = [doc for doc, score in reranked_docs]
            else:
                context_docs = documents[: settings.RERANK_TOP_K]

            # Генерация ответа с контекстом
            logger.info("Генерация ответа с контекстом...")
            response_text = llm_service.generate_with_context(
                question=request.message,
                context=context_docs,
                conversation_history=history,
            )

            # Проверяем что ответ не пустой
            if not response_text or response_text.strip() == "":
                response_text = "Не удалось получить ответ от модели. Пожалуйста, попробуйте еще раз."
                logger.warning("Получен пустой ответ от модели")

            sources = [result["document"][:100] + "..." for result in search_results[:3]]

        # Обновляем историю диалога
        history.extend([
            {"role": "user", "content": request.message},
            {"role": "assistant", "content": response_text},
        ])

        # Ограничиваем историю последними 10 сообщениями
        conversation_store[conversation_id] = history[-10:]

        # Предлагаемые вопросы
        suggested_questions = [
            "Какие документы можно загружать?",
            "Как работает поиск?",
            "Какие форматы файлов поддерживаются?",
        ]

        logger.info(f"Успешно сгенерирован ответ длиной {len(response_text)} символов")

        return ChatResponse(
            response=response_text,
            conversation_id=conversation_id,
            sources=sources,
            suggested_questions=suggested_questions,
        )

    except Exception as e:
        logger.error(f"Ошибка в чате: {e}")
        raise HTTPException(status_code=500, detail=f"Ошибка обработки запроса: {str(e)}")

@router.get("/conversations/{conversation_id}")
async def get_conversation(conversation_id: str):
    """Получить историю диалога"""
    history = conversation_store.get(conversation_id, [])
    return {"conversation_id": conversation_id, "history": history}
