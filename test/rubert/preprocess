import pandas as pd
from transformers import AutoTokenizer
import numpy as np
from datasets import Dataset, ClassLabel, Sequence
from typing import List, Dict

# Загрузка токенизатора для выбранной модели
model_name = "DeepPavlov/rubert-base-cased" 
tokenizer = AutoTokenizer.from_pretrained(model_name)

def parse_annotated_message(text: str, labels_dict: Dict[str, str]) -> List[tuple]:
    """
    Парсит строку вида '[FIO] поехал в город [LOC]' и возвращает список (token, label).
    """
    tokens_and_labels = []
    current_label = "O"
    current_token = ""
    i = 0

    while i < len(text):
        if text[i] == '[':
            # Если начинается новая метка, сохраняем предыдущий токен
            if current_token:
                tokens_and_labels.append((current_token, current_label))
                current_token = ""
            label_end = text.find(']', i)
            if label_end != -1:
                # Проверяем, является ли содержимое квадратных скобок меткой
                potential_label = text[i+1:label_end]
                if potential_label in labels_dict:
                    current_label = labels_dict[potential_label]
                    i = label_end + 1
                    continue
                else:
                    # Если это не метка, трактуем '[' как часть текста
                    current_token += text[i]
            else:
                current_token += text[i]
        elif text[i] == ' ':
            if current_token:
                tokens_and_labels.append((current_token, current_label))
                current_token = ""
            # Сброс метки после токена (метка сохраняется только для сущности)
            if current_label != "O":
                current_label = "O"
        else:
            current_token += text[i]
        i += 1

    if current_token:
        tokens_and_labels.append((current_token, current_label))

    return tokens_and_labels

def tokenize_and_align_labels(examples, label_all_tokens=False):
    """
    Выравнивает метки после токенизации, так как модель разбивает слова на подтокены.
    """
    tokenized_inputs = tokenizer(
        examples["tokens"],
        truncation=True,
        is_split_into_words=True,
        padding=False,
        max_length=512
    )

    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # Специальные токены получают метку -100
            if word_idx is None:
                label_ids.append(-100)
            # Для первого подтокена каждого слова назначаем метку
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            # Для остальных подтокенов того же слова
            else:
                label_ids.append(label[word_idx] if label_all_tokens else -100)
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Основной пайплайн предобработки
def preprocess_data(csv_path: str):
    """
    Основная функция для предобработки CSV-файла.
    """
    df = pd.read_csv(csv_path)
    
    # Определите ваши метки (добавьте все, которые используются)
    unique_labels = ["FIO", "LOC", "DATE"]  # Добавьте все ваши метки
    label2tag = {f"B-{label}": i for i, label in enumerate(unique_labels, 1)}
    label2tag.update({f"I-{label}": i + len(unique_labels) for i, label in enumerate(unique_labels, 1)})
    label2tag["O"] = 0
    tag2label = {v: k for k, v in label2tag.items()}
    
    # Создаем объект ClassLabel
    class_labels = ClassLabel(
        num_classes=len(label2tag),
        names=list(tag2label[i] for i in range(len(tag2tag)))
    )

    all_tokens = []
    all_ner_tags = []

    for _, row in df.iterrows():
        # Парсим размеченное сообщение
        parsed = parse_annotated_message(row['annotated_message'], 
                                       {label: f"B-{label}" for label in unique_labels})
        
        if not parsed:
            continue
            
        tokens, tags = zip(*parsed)
        
        # Конвертируем теги в числовые метки
        numeric_tags = [label2tag.get(tag, 0) for tag in tags]
        
        all_tokens.append(list(tokens))
        all_ner_tags.append(numeric_tags)

    # Создаем Dataset
    raw_dataset = Dataset.from_dict({
        "tokens": all_tokens,
        "ner_tags": all_ner_tags
    })

    # Применяем токенизацию и выравнивание меток
    tokenized_dataset = raw_dataset.map(
        tokenize_and_align_labels,
        batched=True,
        batch_size=1000
    )

    return tokenized_dataset, class_labels, label2tag

if __name__ == "__main__":
    # Использование
    dataset, label_names, tag_mapping = preprocess_data("your_dataset.csv")
    
    # Сохранение обработанных данных
    dataset.save_to_disk("./processed_ner_dataset")
    
    # Сохранение mappingа меток для будущего использования
    import json
    with open("label_mapping.json", "w") as f:
        json.dump(tag_mapping, f, indent=2)
