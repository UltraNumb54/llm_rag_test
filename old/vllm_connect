from openai import OpenAI
import requests
import time

def check_server_ready():
    try:
        response = requests.get("http://localhost:8000/v1/models", timeout=5)
        return response.status_code == 200
    except:
        return False

def wait_for_server(timeout=60):
    print("Ожидание запуска сервера vLLM...")
    start_time = time.time()
    while time.time() - start_time < timeout:
        if check_server_ready():
            print("✓ Сервер готов!")
            return True
        print(".", end="", flush=True)
        time.sleep(2)
    print("\nСервер не запустился за отведенное время")
    return False

# Ждем сервер перед подключением
if wait_for_server():
    client = OpenAI(
        base_url="http://localhost:8000/v1",
        api_key="none"
    )
    
    def ask_model(question, max_tokens=500):
        try:
            response = client.chat.completions.create(
                model="Qwen/Qwen2.5-7B-Instruct-AWQ",
                messages=[
                    {"role": "system", "content": "Ты полезный AI-ассистент. Отвечай на русском языке."},
                    {"role": "user", "content": question}
                ],
                max_tokens=max_tokens,
                temperature=0.7
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Ошибка при запросе к модели: {e}"
    
    # Тестируем
    print(ask_model("Привет! Ответь коротко - ты работаешь?"))
else:
    print("Сервер vLLM не запущен")


# LM Studio

import os
import torch
from langchain_text_splitters import RecursiveCharacterTextSplitter
from openai import OpenAI
from sentence_transformers import SentenceTransformer
import chromadb
from sentence_transformers import CrossEncoder
import numpy as np

os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Используется устройство: {device}")

if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Память GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

embedding_model = SentenceTransformer(
    'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
    device=device
)

reranker = CrossEncoder(
    'cross-encoder/ms-marco-MiniLM-L-6-v2', 
    max_length=512,
    device=device
)

chroma_client = chromadb.PersistentClient(path="./chroma_db")
collection = chroma_client.get_or_create_collection(name="russian_documents")

def generate_embeddings(texts):
    embeddings = embedding_model.encode(
        texts,
        device=device,
        convert_to_tensor=True,
        show_progress_bar=False
    )
    return embeddings.cpu().numpy().tolist()

def search_with_reranking(query, top_k=10, rerank_top_k=5):
    query_embedding = generate_embeddings([query])[0]
    
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    
    if not results['documents']:
        return []
    
    documents = results['documents'][0]
    metadatas = results['metadatas'][0]
    
    pairs = [(query, doc) for doc in documents]
    
    rerank_scores = reranker.predict(
        pairs,
        convert_to_tensor=True,
        show_progress_bar=False
    )
    
    if hasattr(rerank_scores, 'cpu'):
        rerank_scores = rerank_scores.cpu().numpy()
    
    combined_results = list(zip(documents, metadatas, rerank_scores))
    combined_results.sort(key=lambda x: x[2], reverse=True)
    
    return combined_results[:rerank_top_k]

client = OpenAI(
    base_url="http://26.25.60.34:1234/v1",
    api_key="not-needed"
)

def ask_mistral(question, context_documents):
    context = "\n\n".join([doc[0] for doc in context_documents])
    
    prompt = f"""Используй только приведённый ниже контекст для ответа на вопрос. Если в контексте нет информации для ответа, скажи об этом.

Контекст:
{context}

Вопрос: {question}"""

    try:
        response = client.chat.completions.create(
            model="local-model",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=500,
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Произошла ошибка при запросе к LM Studio: {e}"

def rag_qa_pipeline(user_query):
    print(f"\nОбработка запроса: '{user_query}'")
    
    search_results = search_with_reranking(user_query, top_k=5, rerank_top_k=3)
    
    if not search_results:
        return "К сожалению, в моих документах нет информации для ответа на этот вопрос."
    
    print(f"Найдено релевантных фрагментов: {len(search_results)}")
    
    answer = ask_mistral(user_query, search_results)
    return answer

# ===== ТЕСТОВЫЕ ВОПРОСЫ =====
test_questions = [
    "основные идеи документа",
    "о чем этот текст?",
    "ключевые моменты"
]

print("=== ТЕСТ RAG СИСТЕМЫ ===")
print(f"Векторная база содержит {collection.count()} записей")

for question in test_questions:
    print(f"ВОПРОС: {question}")
    
    answer = rag_qa_pipeline(question)
    print(f"\nОТВЕТ: {answer}")
    print('')
