import chromadb
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Используется устройство: {device}")

def initialize_chroma_client():
    try:
        client = chromadb.PersistentClient(path="./chroma_db")
        collections = client.list_collections()
        logger.info(f"Успешное подключение. Коллекций: {len(collections)}")
        
        try:
            collection = client.get_collection(name="docs")
            logger.info(f"Коллекция 'docs' загружена. Документов: {collection.count()}")
        except Exception:
            logger.info("Создание новой коллекции 'docs'")
            collection = client.create_collection(name="docs")
            
        return client, collection
        
    except Exception as e:
        logger.error(f"Ошибка инициализации ChromaDB: {e}")
        raise

chroma_client, collection = initialize_chroma_client()

embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

reranker_model_name = "Alibaba-NLP/gte-multilingual-reranker-base"
tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)
reranker = AutoModelForSequenceClassification.from_pretrained(
    reranker_model_name, 
    trust_remote_code=True,
    torch_dtype=torch.float16
).to(device)
reranker.eval()

def generate_embeddings(texts):
    return embedding_model.encode(texts, convert_to_tensor=True).cpu().numpy()

def search_with_detailed_logging(query, top_k=10, rerank_top_k=5):
    print(f"\n=== ПОИСК: '{query}' ===")
    
    print("1. Генерация эмбеддинга запроса...")
    query_embedding = generate_embeddings([query])[0]
    
    print(f"2. Векторный поиск (top_k={top_k})...")
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    
    if not results['documents']:
        print("Не найдено документов")
        return []
    
    documents = results['documents'][0]
    metadatas = results['metadatas'][0]
    distances = results['distances'][0] if 'distances' in results else [None] * len(documents)
    
    print("\n3. РЕЗУЛЬТАТЫ ВЕКТОРНОГО ПОИСКА:")
    print("-" * 50)
    for i, (doc, meta, dist) in enumerate(zip(documents, metadatas, distances)):
        print(f"{i+1}. [Расстояние: {dist:.4f}] Источник: {meta['source']}")
        print(f"   Фрагмент: {doc[:100]}...")
        print()
    
    print("4. Реранкинг с cross-encoder...")
    pairs = [(query, doc) for doc in documents]
    
    with torch.no_grad():
        inputs = tokenizer(pairs, padding=True, truncation=True, 
                          return_tensors='pt', max_length=512).to(device)
        scores = reranker(**inputs, return_dict=True).logits.view(-1, ).float()
        rerank_scores = scores.cpu().numpy()
    
    combined_results = list(zip(documents, metadatas, distances, rerank_scores))
    combined_results.sort(key=lambda x: x[3], reverse=True)
    
    print("\n5. РЕЗУЛЬТАТЫ ПОСЛЕ РЕРАНКИНГА:")
    print("-" * 50)
    for i, (doc, meta, dist, score) in enumerate(combined_results[:rerank_top_k]):
        print(f"{i+1}. [Реранк: {score:.4f}] [Вектор: {dist:.4f}] Источник: {meta['source']}")
        print(f"   Фрагмент: {doc[:100]}...")
        print()
    
    return combined_results[:rerank_top_k]

def search_with_reranking(query, top_k=10, rerank_top_k=5):
    results = search_with_detailed_logging(query, top_k, rerank_top_k)
    return [(doc, meta, score) for doc, meta, dist, score in results] if results else []

if __name__ == "__main__":
    test_query = "пример запроса на русском"
    search_with_reranking(test_query)
