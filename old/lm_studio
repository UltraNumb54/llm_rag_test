# LM Studio
import os
import torch
from langchain_text_splitters import RecursiveCharacterTextSplitter
from openai import OpenAI
from sentence_transformers import SentenceTransformer
import chromadb
from sentence_transformers import CrossEncoder
import numpy as np

os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Используется устройство: {device}")

if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Память GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

embedding_model = SentenceTransformer(
    'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
    device=device
)

reranker = CrossEncoder(
    'cross-encoder/ms-marco-MiniLM-L-6-v2', 
    max_length=512,
    device=device
)

chroma_client = chromadb.PersistentClient(path="./chroma_db")
collection = chroma_client.get_or_create_collection(name="docs")

def load_and_process_documents(folder_path, chunk_size=500, chunk_overlap=50):
    """Загружает txt файлы из папки и разбивает на чанки"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    
    documents = []
    metadatas = []
    
    # Чтение всех txt файлов в указанной папке
    for filename in os.listdir(folder_path):
        if filename.endswith('.txt'):
            file_path = os.path.join(folder_path, filename)
            try:
                with open(file_path, 'r', encoding='utf-8') as file:
                    content = file.read()
                
                # Разбиваем текст на чанки
                chunks = text_splitter.split_text(content)
                
                # Добавляем каждый чанк с метаданными
                for i, chunk in enumerate(chunks):
                    documents.append(chunk)
                    metadatas.append({
                        'source': filename,
                        'chunk_id': i,
                        'file_path': file_path
                    })
                    
                print(f"Обработан файл: {filename} - {len(chunks)} чанков")
                
            except Exception as e:
                print(f"Ошибка при обработке файла {filename}: {e}")
    
    return documents, metadatas

def add_documents_to_collection(folder_path):
    """Добавляет документы в векторную базу"""
    print("Начало загрузки документов...")
    
    # Загружаем и обрабатываем документы
    documents, metadatas = load_and_process_documents(folder_path)
    
    if not documents:
        print("Не найдено txt файлов для обработки")
        return
    
    print(f"Всего чанков для обработки: {len(documents)}")
    
    # Генерируем эмбеддинги
    print("Генерация эмбеддингов...")
    embeddings = generate_embeddings(documents)
    
    # Создаем ID для каждого документа
    ids = [f"doc_{i}" for i in range(len(documents))]
    
    # Добавляем в коллекцию
    print("Добавление в векторную базу...")
    collection.add(
        embeddings=embeddings,
        documents=documents,
        metadatas=metadatas,
        ids=ids
    )
    
    print(f"Успешно добавлено {len(documents)} документов в базу данных")

def generate_embeddings(texts):
    embeddings = embedding_model.encode(
        texts,
        device=device,
        convert_to_tensor=True,
        show_progress_bar=True  # Включим для отслеживания прогресса
    )
    return embeddings.cpu().numpy().tolist()

def search_with_reranking(query, top_k=10, rerank_top_k=5):
    query_embedding = generate_embeddings([query])[0]
    
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    
    if not results['documents']:
        return []
    
    documents = results['documents'][0]
    metadatas = results['metadatas'][0]
    
    pairs = [(query, doc) for doc in documents]
    
    rerank_scores = reranker.predict(
        pairs,
        convert_to_tensor=True,
        show_progress_bar=False
    )
    
    if hasattr(rerank_scores, 'cpu'):
        rerank_scores = rerank_scores.cpu().numpy()
    
    combined_results = list(zip(documents, metadatas, rerank_scores))
    combined_results.sort(key=lambda x: x[2], reverse=True)
    
    return combined_results[:rerank_top_k]

client = OpenAI(
    base_url="http://26.25.60.34:1234/v1",
    api_key="not-needed"
)

def ask_mistral(question, context_documents):
    context = "\n\n".join([doc[0] for doc in context_documents])
    
    prompt = f"""Используй только приведённый ниже контекст для ответа на вопрос. Если в контексте нет информации для ответа, скажи об этом.

Контекст:
{context}

Вопрос: {question}"""

    try:
        response = client.chat.completions.create(
            model="local-model",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=500,
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Произошла ошибка при запросе к LM Studio: {e}"

def rag_qa_pipeline(user_query):
    print(f"\nОбработка запроса: '{user_query}'")
    
    search_results = search_with_reranking(user_query, top_k=5, rerank_top_k=3)
    
    if not search_results:
        return "К сожалению, в моих документах нет информации для ответа на этот вопрос."
    
    print(f"Найдено релевантных фрагментов: {len(search_results)}")
    
    answer = ask_mistral(user_query, search_results)
    return answer

# ===== ЗАГРУЗКА ДОКУМЕНТОВ =====
# Укажите путь к папке с txt файлами
documents_folder = "./documents"  # измените на путь к вашей папке

# Проверяем, есть ли уже документы в базе
if collection.count() == 0:
    print("База данных пуста. Загружаем документы...")
    
    # Создаем папку для документов, если её нет
    if not os.path.exists(documents_folder):
        os.makedirs(documents_folder)
        print(f"Создана папка {documents_folder}. Добавьте туда txt файлы и перезапустите скрипт.")
    else:
        # Загружаем документы в базу
        add_documents_to_collection(documents_folder)
else:
    print(f"В базе данных уже есть {collection.count()} документов")

test_questions = []

print("\n=== ТЕСТ RAG СИСТЕМЫ ===")
print(f"Векторная база содержит {collection.count()} записей")

if collection.count() > 0:
    for question in test_questions:
        print(f"\nВОПРОС: {question}")
        
        answer = rag_qa_pipeline(question)
        print(f"ОТВЕТ: {answer}")
        print('-' * 50)
else:
    print("База данных пуста. Добавьте txt файлы в папку documents и перезапустите скрипт.")
