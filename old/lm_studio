import chromadb
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
import logging
from openai import OpenAI
import os

# Настройка логирования
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Используется устройство: {device}")

# Инициализация ChromaDB
def initialize_chroma_client():
    try:
        client = chromadb.PersistentClient(path="./chroma_db")
        collections = client.list_collections()
        logger.info(f"Успешное подключение. Коллекций: {len(collections)}")
        
        try:
            collection = client.get_collection(name="docs")
            logger.info(f"Коллекция 'docs' загружена. Документов: {collection.count()}")
        except Exception:
            logger.info("Создание новой коллекции 'docs'")
            collection = client.create_collection(name="docs")
            
        return client, collection
        
    except Exception as e:
        logger.error(f"Ошибка инициализации ChromaDB: {e}")
        raise

chroma_client, collection = initialize_chroma_client()

# Модели для эмбеддингов и реранкинга
embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

reranker_model_name = "Alibaba-NLP/gte-multilingual-reranker-base"
tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)
reranker = AutoModelForSequenceClassification.from_pretrained(
    reranker_model_name, 
    trust_remote_code=True,
    torch_dtype=torch.float16
).to(device)
reranker.eval()

# Клиент для LM Studio
lm_client = OpenAI(
    base_url="http://26.25.60.34:1234/v1",
    api_key="not-needed"
)

def generate_embeddings(texts):
    return embedding_model.encode(texts, convert_to_tensor=True).cpu().numpy()

def search_with_detailed_logging(query, top_k=10, rerank_top_k=5):
    print(f"\n=== ПОИСК: '{query}' ===")
    
    print("1. Генерация эмбеддинга запроса...")
    query_embedding = generate_embeddings([query])[0]
    
    print(f"2. Векторный поиск (top_k={top_k})...")
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    
    if not results['documents']:
        print("Не найдено документов")
        return []
    
    documents = results['documents'][0]
    metadatas = results['metadatas'][0]
    distances = results['distances'][0] if 'distances' in results else [None] * len(documents)
    
    print("\n3. РЕЗУЛЬТАТЫ ВЕКТОРНОГО ПОИСКА:")
    print("-" * 50)
    for i, (doc, meta, dist) in enumerate(zip(documents, metadatas, distances)):
        print(f"{i+1}. [Расстояние: {dist:.4f}] Источник: {meta['source']}")
        print(f"   Фрагмент: {doc[:100]}...")
        print()
    
    print("4. Реранкинг с cross-encoder...")
    pairs = [(query, doc) for doc in documents]
    
    with torch.no_grad():
        inputs = tokenizer(pairs, padding=True, truncation=True, 
                          return_tensors='pt', max_length=512).to(device)
        scores = reranker(**inputs, return_dict=True).logits.view(-1, ).float()
        rerank_scores = scores.cpu().numpy()
    
    combined_results = list(zip(documents, metadatas, distances, rerank_scores))
    combined_results.sort(key=lambda x: x[3], reverse=True)
    
    print("\n5. РЕЗУЛЬТАТЫ ПОСЛЕ РЕРАНКИНГА:")
    print("-" * 50)
    for i, (doc, meta, dist, score) in enumerate(combined_results[:rerank_top_k]):
        print(f"{i+1}. [Реранк: {score:.4f}] [Вектор: {dist:.4f}] Источник: {meta['source']}")
        print(f"   Фрагмент: {doc[:100]}...")
        print()
    
    return combined_results[:rerank_top_k]

def ask_llm(question, context_documents, model="local-model"):
    """Генерация ответа от LLM на основе найденных документов"""
    print("\n6. ГЕНЕРАЦИЯ ОТВЕТА ОТ LLM...")
    
    # Собираем контекст из найденных документов
    context = "\n\n".join([f"Источник: {doc[1]['source']}\nСодержание: {doc[0]}" 
                          for doc in context_documents])
    
    prompt = f"""Используй только приведённый ниже контекст для ответа на вопрос. 
Если в контексте нет информации для ответа, скажи "В предоставленных документах нет информации для ответа на этот вопрос".

Контекст:
{context}

Вопрос: {question}

Ответ:"""
    
    try:
        print("Отправка запроса к LM Studio...")
        response = lm_client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "Ты - полезный ассистент, который отвечает на вопросы на основе предоставленного контекста."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
            max_tokens=500,
        )
        
        answer = response.choices[0].message.content
        print("Ответ успешно сгенерирован")
        return answer
        
    except Exception as e:
        error_msg = f"Ошибка при запросе к LM Studio: {e}"
        logger.error(error_msg)
        return error_msg

def full_rag_pipeline(query, top_k=10, rerank_top_k=3):
    """Полный RAG пайплайн: поиск + реранкинг + генерация ответа"""
    
    # Поиск и реранкинг
    search_results = search_with_detailed_logging(query, top_k, rerank_top_k)
    
    if not search_results:
        return "К сожалению, в моих документах нет информации для ответа на этот вопрос."
    
    # Генерация ответа от LLM
    final_results = [(doc, meta, score) for doc, meta, dist, score in search_results]
    answer = ask_llm(query, final_results)
    
    print("\n" + "="*70)
    print("ФИНАЛЬНЫЙ ОТВЕТ:")
    print("="*70)
    print(answer)
    print("="*70)
    
    return answer

# Тестирование
if __name__ == "__main__":
    test_queries = [
        "Что такое искусственный интеллект?",
        # Добавьте свои тестовые запросы здесь
    ]
    
    for query in test_queries:
        full_rag_pipeline(query)

# Простой вызов
answer = full_rag_pipeline("Ваш вопрос здесь")

# С кастомными параметрами
answer = full_rag_pipeline(
    query="Сложный вопрос", 
    top_k=15, 
    rerank_top_k=5
)


------------------------

def rag_qa_pipeline(user_query):
    print(f"\nОбработка запроса: '{user_query}'")
    
    # Используем top_k=5 и rerank_top_k=3 как раньше, или измените по необходимости
    search_results = search_with_reranking(user_query, top_k=5, rerank_top_k=3)
    
    if not search_results:
        print("Поиск не вернул результатов.")
        return "К сожалению, в моих документах нет информации для ответа на этот вопрос."
    
    print(f"--- Найдено {len(search_results)} релевантных фрагментов после rerank ---")
    for idx, (doc, metadata, score) in enumerate(search_results):
        print(f"  Результат {idx + 1} (Score: {score:.4f}):")
        print(f"    Источник: {metadata.get('source', 'N/A')}")
        print(f"    Фрагмент: {doc[:200]}...") # Выводим первые 200 символов фрагмента
        print("-" * 20)
    
    # Формируем контекст из rerank'нутых результатов
    context_documents = search_results # search_results уже содержит (doc, metadata, score)
    
    answer = ask_mistral(user_query, context_documents)
    return answer

# reranker = CrossEncoder(
#     'cross-encoder/ms-marco-MiniLM-L-6-v2',
#     max_length=512,
#     device=device
# )

reranker = CrossEncoder(
    'DiTy/cross-encoder-russian-msmarco',
    max_length=512,
    device=device
)

# LM Studio
import os
import torch
from langchain_text_splitters import RecursiveCharacterTextSplitter
from openai import OpenAI
from sentence_transformers import SentenceTransformer
import chromadb
from sentence_transformers import CrossEncoder
import numpy as np

os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Используется устройство: {device}")

if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Память GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

embedding_model = SentenceTransformer(
    'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
    device=device
)

reranker = CrossEncoder(
    'cross-encoder/ms-marco-MiniLM-L-6-v2', 
    max_length=512,
    device=device
)

chroma_client = chromadb.PersistentClient(path="./chroma_db")
collection = chroma_client.get_or_create_collection(name="docs")

def load_and_process_documents(folder_path, chunk_size=500, chunk_overlap=50):
    """Загружает txt файлы из папки и разбивает на чанки"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    
    documents = []
    metadatas = []
    
    # Чтение всех txt файлов в указанной папке
    for filename in os.listdir(folder_path):
        if filename.endswith('.txt'):
            file_path = os.path.join(folder_path, filename)
            try:
                with open(file_path, 'r', encoding='utf-8') as file:
                    content = file.read()
                
                # Разбиваем текст на чанки
                chunks = text_splitter.split_text(content)
                
                # Добавляем каждый чанк с метаданными
                for i, chunk in enumerate(chunks):
                    documents.append(chunk)
                    metadatas.append({
                        'source': filename,
                        'chunk_id': i,
                        'file_path': file_path
                    })
                    
                print(f"Обработан файл: {filename} - {len(chunks)} чанков")
                
            except Exception as e:
                print(f"Ошибка при обработке файла {filename}: {e}")
    
    return documents, metadatas

def add_documents_to_collection(folder_path):
    """Добавляет документы в векторную базу"""
    print("Начало загрузки документов...")
    
    # Загружаем и обрабатываем документы
    documents, metadatas = load_and_process_documents(folder_path)
    
    if not documents:
        print("Не найдено txt файлов для обработки")
        return
    
    print(f"Всего чанков для обработки: {len(documents)}")
    
    # Генерируем эмбеддинги
    print("Генерация эмбеддингов...")
    embeddings = generate_embeddings(documents)
    
    # Создаем ID для каждого документа
    ids = [f"doc_{i}" for i in range(len(documents))]
    
    # Добавляем в коллекцию
    print("Добавление в векторную базу...")
    collection.add(
        embeddings=embeddings,
        documents=documents,
        metadatas=metadatas,
        ids=ids
    )
    
    print(f"Успешно добавлено {len(documents)} документов в базу данных")

def generate_embeddings(texts):
    embeddings = embedding_model.encode(
        texts,
        device=device,
        convert_to_tensor=True,
        show_progress_bar=True  # Включим для отслеживания прогресса
    )
    return embeddings.cpu().numpy().tolist()

def search_with_reranking(query, top_k=10, rerank_top_k=5):
    query_embedding = generate_embeddings([query])[0]
    
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    
    if not results['documents']:
        return []
    
    documents = results['documents'][0]
    metadatas = results['metadatas'][0]
    
    pairs = [(query, doc) for doc in documents]
    
    rerank_scores = reranker.predict(
        pairs,
        convert_to_tensor=True,
        show_progress_bar=False
    )
    
    if hasattr(rerank_scores, 'cpu'):
        rerank_scores = rerank_scores.cpu().numpy()
    
    combined_results = list(zip(documents, metadatas, rerank_scores))
    combined_results.sort(key=lambda x: x[2], reverse=True)
    
    return combined_results[:rerank_top_k]

client = OpenAI(
    base_url="http://26.25.60.34:1234/v1",
    api_key="not-needed"
)

def ask_mistral(question, context_documents):
    context = "\n\n".join([doc[0] for doc in context_documents])
    
    prompt = f"""Используй только приведённый ниже контекст для ответа на вопрос. Если в контексте нет информации для ответа, скажи об этом.

Контекст:
{context}

Вопрос: {question}"""

    try:
        response = client.chat.completions.create(
            model="local-model",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=500,
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Произошла ошибка при запросе к LM Studio: {e}"

def rag_qa_pipeline(user_query):
    print(f"\nОбработка запроса: '{user_query}'")
    
    search_results = search_with_reranking(user_query, top_k=5, rerank_top_k=3)
    
    if not search_results:
        return "К сожалению, в моих документах нет информации для ответа на этот вопрос."
    
    print(f"Найдено релевантных фрагментов: {len(search_results)}")
    
    answer = ask_mistral(user_query, search_results)
    return answer

# ===== ЗАГРУЗКА ДОКУМЕНТОВ =====
# Укажите путь к папке с txt файлами
documents_folder = "./documents"  # измените на путь к вашей папке

# Проверяем, есть ли уже документы в базе
if collection.count() == 0:
    print("База данных пуста. Загружаем документы...")
    
    # Создаем папку для документов, если её нет
    if not os.path.exists(documents_folder):
        os.makedirs(documents_folder)
        print(f"Создана папка {documents_folder}. Добавьте туда txt файлы и перезапустите скрипт.")
    else:
        # Загружаем документы в базу
        add_documents_to_collection(documents_folder)
else:
    print(f"В базе данных уже есть {collection.count()} документов")

test_questions = []

print("\n=== ТЕСТ RAG СИСТЕМЫ ===")
print(f"Векторная база содержит {collection.count()} записей")

if collection.count() > 0:
    for question in test_questions:
        print(f"\nВОПРОС: {question}")
        
        answer = rag_qa_pipeline(question)
        print(f"ОТВЕТ: {answer}")
        print('-' * 50)
else:
    print("База данных пуста. Добавьте txt файлы в папку documents и перезапустите скрипт.")
