
----------------

vllm serve Qwen/Qwen3-8B --gpu-memory-utilization 0.85 --max-model-len 8192
диск: ~/.cache/huggingface/hub/
# В терминале WSL2
source ~/.venv/bin/activate  # если используете venv
vllm serve Qwen/Qwen3-8B --gpu-memory-utilization 0.85

# В том же терминале где запущен сервер нажмите:
Ctrl + C
# Найти процесс vLLM
ps aux | grep vllm
# Завершить процесс
kill [PID_номер]

from openai import OpenAI
import time

# Подключаемся к локальному серверу vLLM
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="none"  # vLLM не требует аутентификации по умолчанию
)

def ask_model(question, max_tokens=500):
    try:
        response = client.chat.completions.create(
            model="Qwen/Qwen3-8B",
            messages=[
                {"role": "system", "content": "Ты полезный AI-ассистент. Отвечай на русском языке."},
                {"role": "user", "content": question}
            ],
            max_tokens=max_tokens,
            temperature=0.7
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Ошибка: {e}"

# Примеры использования в Jupyter
print(ask_model("Объясни квантовую механику простыми словами"))
print(ask_model("Напиши код на Python для сортировки списка"))

через http запрос:
import requests

def simple_chat(prompt):
    response = requests.post(
        "http://localhost:8000/v1/chat/completions",
        json={
            "model": "Qwen/Qwen3-8B",
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 300
        }
    )
    return response.json()["choices"][0]["message"]["content"]

Проверить использование VRAM:
nvidia-smi
Проверить, работает ли сервер:
curl http://localhost:8000/v1/models

