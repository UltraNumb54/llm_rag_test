from openai import OpenAI

# Клиент для подключения к локальному серверу vLLM
client = OpenAI(
    api_key="EMPTY",
    base_url="http://localhost:8000/v1"
)

response = client.chat.completions.create(
    model="Qwen/Qwen3-8B",
    messages=[
        {"role": "system", "content": "Ты полезный помощник, который отвечает на русском языке."},
        {"role": "user", "content": "Объясни, что такое большая языковая модель, простыми словами."}
    ],
    max_tokens=300,
    temperature=0.7
)

print(response.choices[0].message.content)
