import os
import torch
import time
from langchain_text_splitters import RecursiveCharacterTextSplitter
from openai import OpenAI
from sentence_transformers import SentenceTransformer
import chromadb
from sentence_transformers import CrossEncoder
import numpy as np
from fastapi import FastAPI, HTTPException, Request
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
import uvicorn
from typing import List, Optional
import glob

app = FastAPI(title="RAG Chat", description="Тестовый чат с документами")

app.mount("/static", StaticFiles(directory="app/static"), name="static")

# Модели для запросов
class QueryRequest(BaseModel):
    question: str
    top_k: int = 10
    rerank_top_k: int = 5

class IngestRequest(BaseModel):
    documents_path: str
    chunk_size: int = 500
    chunk_overlap: int = 50

class ConfigRequest(BaseModel):
    model_base_url: str
    model_api_key: str = "not-needed"

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    message: str
    history: List[ChatMessage] = []

embedding_model = None
reranker = None
chroma_client = None
collection = None
client = None
device = None

def initialize_models():
    global embedding_model, reranker, chroma_client, collection, client, device
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Используется устройство: {device}")

    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"Память GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

    # Инициализация моделей
    embedding_model = SentenceTransformer(
        'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
        device=device
    )

    reranker = CrossEncoder(
        'cross-encoder/ms-marco-MiniLM-L-6-v2', 
        max_length=512,
        device=device
    )

    # Инициализация ChromaDB
    chroma_client = chromadb.PersistentClient(path="./chroma_db")
    collection = chroma_client.get_or_create_collection(name="docs_test")

    client = None

def generate_embeddings(texts):
    embeddings = embedding_model.encode(
        texts,
        device=device,
        convert_to_tensor=True,
        show_progress_bar=True
    )
    return embeddings.cpu().numpy().tolist()

def load_documents_from_directory(directory_path: str):
    """Загрузка документов из директории"""
    supported_extensions = ['*.txt', '*.pdf', '*.docx', '*.md']
    documents = []
    
    for extension in supported_extensions:
        pattern = os.path.join(directory_path, '**', extension) if extension == '*.pdf' else os.path.join(directory_path, extension)
        files = glob.glob(pattern, recursive=True)
        
        for file_path in files:
            try:
                if file_path.endswith('.txt'):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        documents.append({
                            'content': content,
                            'source': file_path
                        })
                # Обработка других форматов
                # elif file_path.endswith('.pdf'):
                #     from langchain_community.document_loaders import PyPDFLoader
                #     loader = PyPDFLoader(file_path)
                #     pages = loader.load()
                #     for page in pages:
                #         documents.append({
                #             'content': page.page_content,
                #             'source': file_path
                #         })
            except Exception as e:
                print(f"Ошибка загрузки файла {file_path}: {e}")
    
    return documents

@app.on_event("startup")
async def startup_event():
    initialize_models()
    print("API сервер запущен и готов к работе")
    print("Веб-интерфейс доступен по адресу: http://localhost:8000")

@app.get("/", response_class=HTMLResponse)
async def read_root():
    """Главная страница с веб-интерфейсом"""
    with open("app/static/index.html", "r", encoding="utf-8") as f:
        html_content = f.read()
    return HTMLResponse(content=html_content)

@app.post("/configure_model")
async def configure_model(config: ConfigRequest):
    """Настройка подключения к модели"""
    global client
    try:
        client = OpenAI(
            base_url=config.model_base_url,
            api_key=config.model_api_key
        )
        # Тестируем подключение
        client.models.list()
        return {"status": "success", "message": f"Модель настроена на {config.model_base_url}"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ошибка подключения к модели: {e}")

@app.post("/ingest_documents")
async def ingest_documents(ingest: IngestRequest):
    """Загрузка документов в векторную БД"""
    if not os.path.exists(ingest.documents_path):
        raise HTTPException(status_code=400, detail="Указанный путь не существует")
    
    try:
        # Загрузка документов
        documents = load_documents_from_directory(ingest.documents_path)
        if not documents:
            return {"status": "warning", "message": "Не найдено документов для обработки"}
        
        # Разбивка на чанки
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=ingest.chunk_size,
            chunk_overlap=ingest.chunk_overlap
        )
        
        all_chunks = []
        for doc in documents:
            chunks = text_splitter.split_text(doc['content'])
            for chunk in chunks:
                all_chunks.append({
                    'content': chunk,
                    'source': doc['source']
                })
        
        # Генерация эмбеддингов и сохранение в ChromaDB
        texts = [chunk['content'] for chunk in all_chunks]
        embeddings = generate_embeddings(texts)
        
        # Добавление в коллекцию
        collection.add(
            embeddings=embeddings,
            documents=texts,
            metadatas=[{"source": chunk['source']} for chunk in all_chunks],
            ids=[f"doc_{i}" for i in range(len(texts))]
        )
        
        return {
            "status": "success", 
            "message": f"Загружено {len(documents)} документов, создано {len(texts)} чанков",
            "documents_count": len(documents),
            "chunks_count": len(texts)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ошибка обработки документов: {e}")

@app.post("/ask")
async def ask_question(query: QueryRequest):
    """Задать вопрос системе"""
    if client is None:
        raise HTTPException(status_code=400, detail="Сначала настройте модель через /configure_model")
    
    if collection.count() == 0:
        raise HTTPException(status_code=400, detail="Векторная БД пуста. Сначала загрузите документы через /ingest_documents")
    
    try:
        # Поиск с переранжированием
        query_embedding = generate_embeddings([query.question])[0]
        
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=query.top_k
        )
        
        if not results['documents']:
            return {"answer": "К сожалению, в моих документах нет информации для ответа на этот вопрос."}
        
        documents = results['documents'][0]
        metadatas = results['metadatas'][0]
        
        pairs = [(query.question, doc) for doc in documents]
        rerank_scores = reranker.predict(pairs, show_progress_bar=False)
        
        if hasattr(rerank_scores, 'cpu'):
            rerank_scores = rerank_scores.cpu().numpy()
        
        combined_results = list(zip(documents, metadatas, rerank_scores))
        combined_results.sort(key=lambda x: x[2], reverse=True)
        final_results = combined_results[:query.rerank_top_k]
        
        # Генерация ответа
        context = "\n\n".join([doc[0] for doc in final_results])
        
        prompt = f"""Используй только приведённый ниже контекст для ответа на вопрос. Если в контексте нет информации для ответа, скажи об этом.

Контекст:
{context}

Вопрос: {query.question}"""

        response = client.chat.completions.create(
            model="local-model",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=500,
        )
        
        answer = response.choices[0].message.content
        
        return {
            "question": query.question,
            "answer": answer,
            "sources": [{"content": doc[0], "source": doc[1]['source'], "score": float(doc[2])} for doc in final_results],
            "total_chunks_in_db": collection.count()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ошибка обработки запроса: {e}")

@app.post("/chat")
async def chat_endpoint(chat_request: ChatRequest):
    """Упрощенный endpoint для чата"""
    if client is None:
        raise HTTPException(status_code=400, detail="Сначала настройте модель через /configure_model")
    
    # Используем существующую логику
    query = QueryRequest(question=chat_request.message)
    response = await ask_question(query)
    
    return {
        "response": response["answer"],
        "sources": response.get("sources", [])
    }

@app.get("/status")
async def get_status():
    """Получить статус системы"""
    return {
        "device": device,
        "collection_size": collection.count() if collection else 0,
        "model_configured": client is not None,
        "embedding_model_ready": embedding_model is not None
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
